{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "Once the parameter space is well defined and data has been collected, it is the aim to estimate number of sources and the source parameters from the given parameter space as an unsupervised learning task, with no prior information given on the unknown parameters.\n",
    "The problem of finding clusters in a set of data points can be approached by using probabilistic techniques or non-probabilistic techniques. An example of a non-probabilistic clustering technique is the $k$-means algorithm (Lloyd, 1982). The immediate $k$-means algorithm does need an input that specifies the number of clusters to estimate. In this study we have modelled the panning parameters using the probabilistic clustering technique, as a mixture of Gaussians. We have found that the $K$-means is a good algorithm for initializing the EM-algorithm in the given context.\n",
    "\n",
    "## $K$-means clustering\n",
    "We consider the problem of identifying clusters of data points in a multidimensional space. We observe $N$ observations of the data set {$\\mathbf{x}_1,\\dots,\\mathbf{x}_N$}. In general the variable $\\mathbf{x}$ is $D$-dimensional. However, we have defined two parameters in this study ($D=2$). Each cluster center is represented by $\\mathbf{\\mu}_k$ after we have assigned each point in the data set to a given cluster. The assignment of a data point $\\mathbf{x}_n$ to cluster $k$ is described by the binary indicator variable $b_{n,k} \\in \\{0,1\\}$. The aim is to minimize the sum of squares distance from each data point to its closest center vector $\\mathbf{\\mu}_k$.  We can now describe a cost function $J$ as,\n",
    "\\begin{equation}\n",
    "    J = \\sum_{n=1}^{N}\\sum_{k=1}^{K} b_{n,k} ||\\mathbf{x}_n-\\mathbf{\\mu}_k||^2\n",
    "\\end{equation}\n",
    "To find the values of {$b_{n,k}$} and {$\\mathbf{\\mu}_k$} to minimize $J$ is done by using an iterative optimization procedure, involving two steps for each iteration. To begin the iterations, some initial values are assigned to $\\mathbf{\\mu}_k$. The two iterative steps are,\n",
    "* Minimize $J$ with respect to $b_{n,k}$, with $\\mathbf{\\mu}_k$ fixed.\n",
    "* Minimize $J$ with respect to $\\mathbf{\\mu}_k$, with $b_{n,k}$ fixed.\n",
    "\n",
    "The assignment of the $n$th data point to the closest cluster center can be expressed as,\n",
    "\\begin{equation}\n",
    "    b_{n,k}  = \n",
    "    \\begin{cases}\n",
    "        1, & \\text{if}\\ k = \\underset{j}{\\arg\\min}\\ ||\\mathbf{x}_n-\\mathbf{\\mu}_j||^2 \\\\\n",
    "        0, & \\text{otherwise}\\\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "Since the cost function $J$ is a quadratic function of $\\mathbf{\\mu}_k$ we differentiate with respect to $\\mathbf{\\mu}_k$ and set it to zero,\n",
    "\\begin{equation}\n",
    "    \\frac{d}{d\\mathbf{\\mu}_k}J = 2 \\sum_{n=1}^{N} b_{n,k} (\\mathbf{x}_n-\\mathbf{\\mu}_k) = 0\n",
    "\\end{equation}\n",
    "and solve for $\\mathbf{\\mu}_k$,\n",
    "\\begin{equation}\n",
    "    \\mathbf{\\mu}_k  = \\frac{ \\sum_{n=1}^{N} b_{n,k}\\mathbf{x}_n }{\\sum_{n=1}^{N} b_{n,k}}\n",
    "\\end{equation}\n",
    "which expressed that $ \\mathbf{\\mu}_k $ is the mean of all data points $\\mathbf{x}_n$ assigned to cluster $k$. The iteration over these two steps are guaranteed to reach convergence. However, it can be stuck in a local minimum rather than the global and it is therefore dependent on the initialization to be well considered. An initialization of the $K$-means have been proposed as the $K$-means++ algorithm by (David Arthur and Sergei Vassilvitskii, 2006), a variant that chooses centers at random from the data points, but weighs the data points according to their squared distance squared from the closest center already\n",
    "chosen. This gives a faster convergence and overcomes some of the local minimum problems. Although the $K$-means clustering algorithm offers no accuracy guarantee, its simplicity is very appealing in practice, thus it is widely used for clustering. The $K$-means assigns every data point uniquely to one cluster, and it is not clear that a data point which is placed midway between two cluster centers is assigned appropriately, but by using probabilistic models such as the Gaussian mixture model (GMM), the assigments can reflect this level of uncertainty.\n",
    "For initialization of the EM-algorithm by deliberately overfitting, i.e. choosing a $K$ much larger than the expected value, the $K$-means algorithm assures that the true parameter are among the estimates, when we later apply model selection to the GMM-model. \n",
    "\n",
    "## Linkage clustering\n",
    "For details on linkage clustering, see Jain et al. (1999). In a nutshell,\n",
    "linkage clustering forms clusters by progressively merging a pair of current clusters. Initially,\n",
    "every data point is a cluster. The two clusters with the minimum pairwise distance are chosen\n",
    "to merge at each step. The procedure is greedy in nature since minimization is conducted sequentially\n",
    "through every merge. Linkage clustering methods differ by the way between-cluster\n",
    "distance is updated when a new cluster is combined from two smaller ones.\n",
    "\n",
    "\n",
    "## Model selection\n",
    "$k$-means can use the criteria of Calinski Harabasz.\n",
    "\n",
    "FIGURE OF CLUSTERING FROM CALINSKI HARABASZ\n",
    "\n",
    "\n",
    "\n",
    "## Model based clustering\n",
    "The $K$-means assigns every data point uniquely to one cluster, and it is not clear that a data point which is placed midway between two cluster centers is assigned appropriately. By using probabilistic models such as the Gaussian mixture model (GMM), the assigments can reflect this level of uncertainty. \n",
    "Using the GMM framework, the full parameter space is modelled as a Gaussian mixture distribution i.e. a linear superposition of Gaussians,\n",
    "\n",
    "\\begin{equation}\n",
    "    p(\\mathbf{x}) = \\sum_{k=1}^{K}  \\alpha_k \\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu},\\mathbf{C})\n",
    "\\end{equation} \n",
    "\\begin{equation}\n",
    "  \tp(\\mathbf{x}) = \\sum_{k=1}^{K} \\alpha_k \\frac{(2\\pi)^{-\\frac{d}{2}} )}{|\\mathbf{C}_k|^\\frac{1}{2}}\\exp{\\bigg\\{ -\\frac{1}{2} (\\mathbf{x}-\\mathbf{\\mu}_k)^T \\mathbf{C}^{-1}_k (\\mathbf{x}-\\mathbf{\\mu}_k) \\bigg\\}}\n",
    "\\end{equation} \n",
    "where $\\mathbf{\\mu}$ is the mean and $\\mathbf{C}$ is the covariance of the $k$th Gaussian. The mixing probabilities $\\{\\alpha_1, \\dots, \\alpha_K\\}$ are constrained to\n",
    "\\begin{equation}\n",
    "    \\sum_{k=1}^{K}  \\alpha_k = 1, \\quad \\quad 0\\leq \\alpha_k \\leq 1\n",
    "\\end{equation} \n",
    "and can be interpretted as the prior probabilities,\n",
    "\n",
    "\\begin{equation}\n",
    "  \tp(\\mathbf{x}) =  \\sum_{k=1}^{K} p(k) p(\\mathbf{x}|\\mathbf{\\theta}_k) = \\sum_{k=1}^{K} \\alpha_k \\frac{(2\\pi)^{-\\frac{d}{2}} )}{|\\mathbf{C}_k|^\\frac{1}{2}}\\exp{\\bigg\\{ -\\frac{1}{2} (\\mathbf{x}-\\mathbf{\\mu}_k)^T \\mathbf{C}^{-1}_k (\\mathbf{x}-\\mathbf{\\mu}_k) \\bigg\\}}\n",
    "\\end{equation}  \n",
    "\n",
    "where each $\\theta_k$ is the parameter specifying the $k$th component. $\\mathbf{\\theta} \\equiv \\{ \\alpha_1, \\dots, \\alpha_K, \\mathbf{\\mu}_1, \\dots, \\mathbf{\\mu}_K\\, \\mathbf{C}_1, \\dots, \\mathbf{C}_K\\}$ specifies the full mixture as the complete set of parameters. \n",
    "Observing a set of $N$ independent distributed samples $\\mathbf{X} =\n",
    "\\{\\mathbf{x}_1 , \\dots , \\mathbf{x}_N \\}$, the log-likelihood function corresponding to a K-source mixture is,\n",
    "\n",
    "\\begin{equation}\\label{eq:loglikelihood}\n",
    "    \\ln p(\\mathbf{X}|\\mathbf{\\alpha},\\mathbf{\\mu},\\mathbf{C}) = \\sum_{n=1}^{N} \\ln\\bigg\\{ \\sum_{k=1}^{K} \\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_k,\\mathbf{C}_k)   \\bigg\\}\n",
    "\\tag{eq:loglikelihood}\n",
    "\\end{equation} \n",
    "Due to the summation inside the logarithm, the maximum likelihood for the parameters no longer have a closed form solution. Therefore, we choose to use the expectation-maximization algorithm. In the following section we proceed with a general description of the EM. \n",
    "#### Fitting the Gaussian mixture\n",
    "Given the data set, the aim is to estimate the corresponding unknown parameter vector $\\mathbf{\\theta}$. However, the data set is unlabelled and it is unknown which component generated each data point, but the $K$-means gives us a way of partitioning points into $K$\n",
    "clusters. Once we have estimated which points go to which cluster,\n",
    "we can estimate a Gaussian mean and covariance for that\n",
    "cluster. It is unlikely that the guess is right the first time, but based on the initial estimates of parameters, it is possible to make\n",
    "a better guess at pairing points with components, in an iterative procedure using the EM-algorithm. \n",
    "\n",
    "Due to the inner sum of (\\ref{eq:loglikelihood}), it is necessary to view the problem by defining a $K$-dimensional binary latent variable $\\mathbf{z}$ that for a given $n$ has $k$ latent variables where only one of these is equal to 1, while the rest are equal to 0. This means that the vector $\\mathbf{z}$ has $K$ possible states and $z_k \\in \\{0,1\\}$ and $\\sum_{k=1}^{K}z_k=1$. If we then view $\\alpha_k$ as the prior probability of $z_k=1$, we can write this distribution in the form,\n",
    "\n",
    "\\begin{equation}\n",
    "    p(\\mathbf{z}) = \\prod_{k=1}^{K} \\alpha_k^{z_k}\n",
    "\\end{equation} \n",
    "The conditional distribution of $\\mathbf{x}$ given $\\mathbf{z}$ is also a Gaussian and can be described by,\n",
    "\n",
    "\\begin{equation}\n",
    "    p(\\mathbf{x}|\\mathbf{z}) = \\prod_{k=1}^{K} \\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_k,\\mathbf{C}_k)^{z_{k}} \n",
    "\\end{equation} \n",
    "\n",
    "We are now able to work with the joint distribution $p(\\mathbf{x},\\mathbf{z}) = \\sum_{\\mathbf{z}} p(\\mathbf{z})p(\\mathbf{x}|\\mathbf{z})$ and the log-likelihood of (\\ref{eq:loglikelihood}) can now be expressed for the complete data set $\\{\\mathbf{X},\\mathbf{Z}\\}$ containg both the observed data $\\mathbf{X}$ and the latent variable $\\mathbf{Z}$ (Bishop, 2006). The log-likelihood is then expressed as,\n",
    "\n",
    "\\begin{equation}\\label{eq:complete_ll}\n",
    "    \\ln p(\\mathbf{X},\\mathbf{Z}|\\mathbf{\\alpha},\\mathbf{\\mu},\\mathbf{C}) = \\sum_{n=1}^{N} \\sum_{k=1}^{K} z_{n,k} \\{\\ln\\{\\alpha_k\\} + \\ln \\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_k,\\mathbf{C}_k) \\}  \n",
    "\\tag{eq:complete_ll}\n",
    "\\end{equation} \n",
    "\n",
    "Since the logarithm acts directly on the Gaussian distribution, it leads to much simpler solution for the maximum likelihood solution. Since we do not know the values of the latent variables, we consider the expectation with respect to the posterior distribution which takes the form,\n",
    "\n",
    "\\begin{equation}\n",
    "    p(\\mathbf{Z}|\\mathbf{X} | \\alpha_k\\mathbf{\\mu}_k,\\mathbf{C}_k) = \\prod_{n=1}^{N} \\prod_{k=1}^{K} (\\alpha_k \\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_k,\\mathbf{C}_k)^{z_{n,k}} \n",
    "\\end{equation} \n",
    "Where $\\alpha_k = \\frac{1}{N} \\sum_{n=1}^{N}z_{n,k}$. \n",
    "The expected value of the complete data log-likelihood is now,\n",
    "\n",
    "\\begin{equation}\\label{eq:exp_complete_ll}\n",
    "   \\mathbb{E}_{\\mathbf{Z}}[\\ln p(\\mathbf{X},\\mathbf{Z}|\\mathbf{\\alpha},\\mathbf{\\mu},\\mathbf{C})] =  \\sum_{n=1}^{N} \\sum_{k=1}^{K} \\gamma(z_{k}) \\{\\ln\\{\\alpha_k\\} + \\ln \\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_k,\\mathbf{C}_k) \\}  \n",
    "\\tag{eq:exp_complete_ll}\n",
    "\\end{equation} \n",
    "Where \n",
    "\\begin{equation}\n",
    "\\gamma(z_{k}) \\equiv p(z_k=1|\\mathbf{x}) = \\mathbb{E}[z_{n,k}] = \\frac{\\alpha_k \\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_k,\\mathbf{C}_k) }{\\sum_{j=1}^{K} \\alpha_j \\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_k,\\mathbf{C}_k)} \n",
    "\\end{equation}\n",
    "\n",
    "It is now possible to proceed with the EM-algorithm to obtain a closed form solution.\n",
    "#### EM-algorithm for the complete data set\n",
    "1. Choose an initial value for the parameter vector $\\mathbf{\\theta}^{\\text{old}}$\n",
    "2. Evaluate $p(\\mathbf{Z}|\\mathbf{X}, \\mathbf{\\theta}^{\\text{old}})$ (E-step).\n",
    "3. Evaluate $\\mathbf{\\theta}^{\\text{new}} = \\sum_{\\mathbf{Z}} p(\\mathbf{Z}|\\mathbf{X} | \\mathbf{\\theta}^{\\text{old}}) \\ln p(\\mathbf{X},\\mathbf{Z} | \\mathbf{\\theta})$ (M-step).\n",
    "4. Check for convergence. If no convergence, then update,\n",
    "$\\mathbf{\\theta}^{\\text{old}} \\leftarrow \\mathbf{\\theta}^{\\text{new}}$ and go to step 2.\n",
    "\n",
    "### Variational Bayes\n",
    "\n",
    "\n",
    "### BIC\n",
    "\n",
    "GRAPH OF monotoneous BIC curve\n",
    "\n",
    "### Mixture minimum description length - MMDL\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Matlab",
   "language": "matlab",
   "name": "matlab"
  },
  "language_info": {
   "codemirror_mode": "octave",
   "file_extension": ".m",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-matlab",
   "name": "matlab",
   "version": "0.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
